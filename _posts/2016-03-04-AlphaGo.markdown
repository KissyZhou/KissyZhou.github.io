---
layout: post
title: Mastering the Game of Go with Deep Neural Networks and Tree Search
date: 2016-03-04 9:00 AM
---
[Silver, et al. 2016](http://www.willamette.edu/~levenick/cs448/goNature.pdf)

## Summary ##

A Supervised Learning policy network p_sigma is learned directly from expert human moves.
This provides immediate feedback and high-quality gradients. This is used for improving board evaluation.
Another network p_pi is trained for fast action selection during rollouts.
An RL policy network p_rho is trained for optimizing winning games through self-play. The reward signal is generated
as the end of the game (+1 for winning, -1 for losing). 
Finally, a value network v_theta is trained to predict the winner of games played by the RL policy against itself. 

## Methodology ##

### Monte Carlo Tree Search ###

AlphaGo uses a form of MCTS for position evaluation. The algorithm for this mixes full rollouts with truncated rollouts, resembling 
TD(gamma). To integrate MCTS into AlphaGo, an asynchronous policy and value MCTS (APV-MCTS) algorithm was developed. 

#### Search Algorithm ####
Each node in the tree stores the prior probability, Monte Carlo estimates of the total action value 
calculated by the rollout policy and the value network, the number of times the rollout policy and 
the value network has evaluated the node, and the combined mean action value. 

* <b>Selection</b> The algorithm begins at the root and uses a variant of UCT to traverse the tree for time steps t < L, where L is 
the step at which the algorithm reaches leaf node sL
* <b>Evaluation</b> The leaf node sL is added to a queue for evaluation by the value network, unless it has previously been evaluated. Then,
the fast rollout policy is used to simulate the rest of the game until a terminal reward is obtained. 
* <b>Backup</b> At each in-tree step t <= L, the rollout statistics are updated as if it has lost n_vl games, to discourage other 
threads from simulateously exploring the identical variation. The value network's output is used to update value 
statistics in a second backward pass through each step t <= L.
At the end of the simulation, the rollout statistics
are updated by replacing the virtual losses by the true outcome. The mean action value (Q(s,a)) is a weighted average
of the output of the value network and the rollout evaluations. 
* <b>Expansion</b> When the visit count exceeds a threshold, the successor state s' = f(s,a) is added to the search
tree. The new node is added to a queue for asynch GPU evaluation by the policy network. 
Prior probabilities are computed by the SL policy network with a softmax temperature set to beta. The threshold
is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate
at which the GPUs evaluate the policy network. Mini-batch sizes of 1 were used to minimize the end-to-end evaluation time. 

A distributed APV-MCTS was implemented which split policy evaluation across worker GPUs and handled 
the in-tree phase of each simulation on worker CPUs. 

### Rollout policy ###

This is a linear softmax policy based on fast, incrementally computed, local pattern-based features. A number of 
handcrafted local features encode common-sense Go rules. Similar to the policy network, the weights pi of the 
rollout policy are trained from 8 million positions from human games to maximize the log likelihood by 
stochastic gradient descent. 3 x 3 pattern matching against previous moves selected by the most probable action 
is used; this is a generalization of the 'last good reply' heuristic. 

### Symmetries ###
Go symmetry is exploited at run-time by dynamically transforming each position using 8 reflections/rotations. 

### Classification policy Network ###
A data set of 29.4 million positions from 160,000 games was used. Each position consisted of a raw board description 
's' and the move 'a' selected by the human. Each position was augmented with all 8 reflections/rotatoins. 
Asynchronous SGD was used to maximize the log likelihood of the action. Step was was initialized to 0.003 and halved
every 80 million steps, without momentum, and mini-batch size of 16. Gradients older than 100 steps were
discarded. 

### RL policy network ###
N games were played in parallel between the current policy network that is being trained an an opponent
that uses parameters from a previous iteration, randomly sampled from a pool of opponents. Games
were played out till termination, scored, then replayed to calucate the policy gradient update. 
REINFORCE was used with a baseline for variance reduction. On the first pass, the baseline was set to 0, 
on the second pass, the value network was used as a baseline. 10,000 minibatches of 128 games was the training set. 

### Value network: Regression ###
Trained to approximate the value function of the RL policy network. Overfitting was handled by crafting
a dataset of uncorrelated self-play positions. Each game was generated by randomly sampling a time-step
U, sampling the first t = 1 to U-1 moves from the SL policy network, then sampling one move 
uniformly at random from available moves, then sampling the remaining sequence of moves until the game terminates, 
t = U + 1 to T, from the RL policy network. Finally, the game is scored to determine the outcome. 

## Evaluation ##
Each program was given a max of 5 s computation time per move. The distributed AlphaGo was used against
Fan Hui and won 5 - 0 in formal matches. 
