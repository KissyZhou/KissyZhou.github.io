---
layout: post
title: "Asynchronous Methods for Deep Reinforcement Learning"
date: 2016-07-30
category: paper-summaries
paper_ref: Mnih, et al., 2016
redirect_from: 
    - /papersummaries/2016/08/02/A3C.html
    - /paper_summaries/2016/08/02/A3C.html
---

[{{ page.paper_ref }}](http://arxiv.org/pdf/1602.01783v1.pdf)

## Summary

The authors presented a number of asynchronous DRL algorithms with the intention of developing RL agents that can be trained on CPUs. 
The algorithms used multiple threads to run copies of the environment and generate uncorrelated sequences of training samples. 
Parameters were then sent to a shared parameter server at regular intervals. Because this promotes non-stationarity for the sequences of SARSA tuples, experience replay is not necessarily needed.
The implementations of RMSProp and Momentum SGD used by the authors employed a Hogwild!-inpsired lock free scheme for maximum efficiency. 
A3C is the "best" agent that was presented in this paper. It is an asynchronous advantage actor-critic algorithm. It maintains an approximation of the
policy, an estimate of the value function, and computes an "advantage" function and a variance-reducing baseline [Degris, et al. 2012](http://icml.cc/2012/papers/268.pdf). An entropy regularization term was also used to discourage premature convergence. 

## Evidence 

State-of-the-art results were obtained on some of the Atari games (ALE). An LSTM-based A3C agent was tested with Deepmind's Labyrinth environment. They also tested on the TORCS car racing environment and MuJoCo, the continuous-space physics simulation engine. 

## Strengths 

Presenting the algorithms in pseudocode is very helpful to the reader. The authors went into implementation details, which is also helpful for those who wish to check the results for themselves.

## Future Directions

* How can we reduce/control the asymptotic variance of these temporal difference methods? Also, of the actor-critic method? (Need to research this topic more).
* Incorprating experience replay (seems a bit self-contradictory)
* Dueling Networks for the state value and advantage functions
* Reducing over-estimation bias of Q-values (Double DQN, etc)
