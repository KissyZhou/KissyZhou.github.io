---
layout: post
title:  "Breaking Down Deep Deterministic Policy Gradients in TensorFlow"
date:   2016-08-17
category: blog_posts
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

---
<center><i>DISCLAIMER: Readers should have a familiarity with basic reinforcement learning concepts and neural networks (e.g., MDPs, policy and value functions, Q-learning, Bellman Equation)</i></center>


## Introduction

Deep Reinforcement Learning has recently gained a lot of traction in the machine learning community due to the significant amount of progress that has been made in the past few years. Traditionally, reinforcement learning algorithms were constrained to tiny, discretized grid worlds, which seriously inhibited them from gaining credibility as being viable machine learning tools. The state and action spaces had to be kept small to avoid the _curse_ _of_ _dimensionality_. After Deep Q-Networks <a href="#References">[4]</a> became a hit, people realized that deep learning methods could be used to finally solve high-dimensional RL problems. One of the subsequent challenges that the RL community faced was figuring out how to deal with continuous action spaces. This is a **big deal**, since most interesting problems in robotic control, etc., fall in this category; [OpenAI Gym](https://gym.openai.com) is a fantastic tool for training and evaluating such reinforcement learning tasks in simulation. Logically, if you discretize your continuous action space too finely, you end up with the same curse of dimensionality as before. On the other hand, a naive descritization of the action space throws away valuable information concerning the geometry of the action domain.

{%
    include image.html
    img="/assets/cartpole.gif"
    caption="CartPole-v0 environment: OpenAI Gym"
%}

Google DeepMind has devised a solid approach to tackling this problem. Building off the prior work of <a href="#References">[3]</a> on Deterministic Policy Gradients, they have produced an **actor-critic** algorithm called Deep Deterministic Policy Gradients (DDPG) <a href="#References">[5]</a> that is **off-policy** and **model-free**, which can learn quite efficiently on problems with continuous action spaces. It uses some of the deep learning tricks that were introduced with Deep Q-Networks (hence the "Deep"-ness of DDPG). Lets break down some of the jargon that was just mentioned...

## First, Some Theory

<a href="#Tensorflow">[I just want to skip to the Tensorflow part]</a>

### Actor-Critic

{%
    include image.html
    img="/assets/actor-critic.png"
    caption="The actor-critic architecture [2]" 
%}

The Actor-Critic learning algorithm is used to represent the policy function independently of the value function. The policy function structure is known as the _actor_, and the value function structure is referred to as the _critic_. The actor produces an action given the current state of the system, and the critic produces a TD (Temporal-Difference) error signal given the state and the action. Therefore, the output of the critic drives learning in both the actor and the critic. In Deep Reinforcement Learning, neural networks can be used to represent the actor and the critic. 

### Off-Policy

Reinforcement Learning algorithms which are characterized as off-policy generally employ a separate behavior policy that is independent of the policy that is evaluated and improved upon in order to simulate sample trajectories. A key benefit of this separation is that the behavior policy can operate by sampling all actions, whereas the estimation policy can be deterministic (e.g., greedy) <a href="#References">[1]</a>. Q-learning is an off-policy algorithm, since it updates the Q values without making any assumptions about the actual policy being followed. Rather, the Q-learning algorithm simply states that the Q-value corresponding to state $s(t)$ and action $a(t)$ is updated using the Q-value of the next state $s(t + 1)$ and the action $a(t + 1)$ that maximizes the Q-value at state $s(t + 1)$. 

On-policy algorithms directly use the policy that is being estimated to sample trajectories during training. 

### Model-free

Model-free RL algorithms are simply those that make no effort at learning the underlying dynamics that govern how an agent interacts with the environment. In the case where the environment has a discrete state space and the agent has a discrete number of actions to choose from, the model would be the transition matrix: $T(s(t + 1)\|s(t), a(t))$. This stochastic matrix gives all of the probabilities for arriving at a desired state given the current state and action. Clearly, for problems with high-dimensional state and action spaces, this matrix is incredibly expensive in space and time to compute. If your state space is the set of all possible 64 x 64 RGB images and your agent has 18 actions available to it, the transition matrix's size is  $\|S \times S \times A\| \approx \|(68.7 \times 10^{9}) \times (68.7 \times 10^9) \times 18\|$, and at 32 bits per matrix element, thats around $3.4 \times 10^{14}$ GB to store it in RAM! 

Rather than dealing with that, model-free algorithms directly estimate the optimal policy or value function through algorithms such as policy iteration or value iteration and (as of late) with function approximators. This is much more efficient computation-wise! However, I should note that, if possible, obtaining and using a good approximation of the underlying model of the system can only be beneficial. Be wary- using a bad approximation of the model of the system will only bring misery. Not using a model means that you need a large number of training examples.

## The Meat and Potatoes of DDPG

At its core, DDPG uses a stochastic behavior policy for good exploration but estimates a **deterministic** target policy, which is much easier to learn. This allows for the use of the Deterministic Policy Gradient theorem (which will be derived shortly). DDPG uses two main neural networks, one for the actor and one for the critic, to compute action predictions for the current state and to generate a temporal-difference (TD) error signal. The input of the actor network is the current state, and the output is a single real value representing an action chosen from a **continuous** space (whoa!). The critic's output is simply the estimated Q-value of the current state and of the action given by the actor. The deterministic policy gradient theorem provides the update rule for the weights of the actor network. The critic network is updated from the gradients obtained from the TD error signal. 

Sadly, it turns out that a vanilla actor-critic algorithm that is off-policy and model-free tends to behave poorly, resisting all of the most valient efforts to get it to converge. The following are most likely some of the key conspirators: 

1. In general, training and evaluating your policy and/or value function with thousands of temporally-correlated simulated trajectories leads to the introduction of enormous amounts of variance in your approximation. The TD error signal is excellent at compounding the variance introduced by your bad predictions over time. It is highly suggested to use a replay buffer of randomly sampled transitions to break up the temporal correlation between training episodes; this is known as **experience replay**. DDPG employs this technique. 

2. Directly updating your actor and critic neural network weights with the gradients obtained from a TD error signal that was computed from your replay buffer and the output of the actor and critic networks causes your learning algorithm to diverge (or to not learn at all). It was recently discovered that using a set of **target networks** to generate the **targets** for your TD error computation regularizes your learning algorithm and increases the stability. Accordingly:

$$
\begin{equation}
y_i = r_i + \gamma Q'(s_{i + 1}, \mu'(s_{i+1}|\theta^{\mu'})|\theta^{Q'})
\end{equation}
$$

$$ 
\begin{equation}
L = \frac{1}{N} \sum_{i} (y_i - Q(s_i, a_i | \theta^{Q})^2) 
\end{equation}
$$

Here, a minibatch of size $N$ has been sampled from the replay buffer, with the $i$ index referring to the i'th sample.  The target for the TD error computation, $y_i$, is computed from the immediate reward and the target actor and critic networks, with weights $\theta^{\mu'}$ and $\theta^{Q'}$ respectively. Then, the critic loss can be computed w.r.t. the output of the critic network for the i'th sample, which is $Q(s_i, a_i \| \theta^{Q})$. 

See <a href="#References">[4]</a> for more details on target networks.

Now, as mentioned above, the weights of the critic neural network can be updated with the gradients derived from the loss function (2). And, remember, the actor network is updated with the Deterministic Policy Gradient. Here lies the crux of Deep Determinstic Policy Gradients! Silver, et al., <a href="#References">[3]</a> proved that the _policy gradient_, which is the gradient of the policy's performance, is equivalent to the deterministic policy gradient: 

$$
\begin{equation}

\nabla_{\theta^{\mu}} \mu \approx \mathbb{E}_{\mu'} \big [ \nabla_{a} Q(s, a|\theta^{Q})|_{s=s_t,a=\mu(s_t)} \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s=s_t} \big ]
\end{equation}
$$ 

**If you would like to move on to the code, feel free to <a href="#Tensorflow">skip ahead</a> here.** Since I think the proof of the deterministic policy gradient theorem is quite nice, I'd like to demonstrate it here before moving on to the code.

<div class="theorem">
(Deterministic Policy Gradient Theorem). 
Suppose the Markov Decision Process satisfies the appropriate conditions (see <a href="#References">[3]</a>). These imply that $\nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})$ and $\nabla_{a} Q(s, a|\theta^{Q})$ exist and that the deterministic policy gradient exists. Then, 

$$
\nabla_{\theta^{\mu}} \mu \approx\int_{S} \rho^{\mu'}(s_t) \nabla_{a} Q(s, a|\theta^{Q})|_{s=s_t, a=\mu(s_t)} \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s=s_t}  ds
$$

$$
\begin{equation}
= \mathbb{E}_{\mu'}  \big[ \nabla_{a} Q(s, a|\theta^{Q})|_{s=s_t, a=\mu(s_t)}  \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s=s_t}  \big]
\end{equation}
$$
</div>

<div class="proof">

For a greedy stochastic policy $ \mu (a | s, \theta)$ over a continuous action space, a global maximization step is required at every time step. Rather, we employ a deterministic policy $ \mu (s | \theta) $ and update the policy parameters by moving them in the direction of the gradient of the action-value function. We take an expectation to average over the suggested directions of improvement from each state w.r.t. the state distribution under the target policy $\mu'$.

$$
\begin{equation}

\theta^{\mu}_{k + 1} = \theta^{\mu}_k + \alpha \mathbb{E}_{\mu'^{k}} \big [ \nabla_{\theta} Q(s, \mu (s|\theta^{\mu}_k)|\theta^{Q}_k)  \big ].

\end{equation}
$$

By applying the chain rule,

$$
\begin{equation}

\theta^{\mu}_{k + 1} = \theta^{\mu}_k + \alpha \mathbb{E}_{\mu'^{k}} \big [ \nabla_{a} Q(s, a|\theta^{Q}_k)|_{a=\mu(s|\theta^{\mu}_k)} \nabla_{\theta} \mu(s|\theta^{\mu}_k) \big ].

\end{equation}
$$
</div>

In <a href="#References">[3]</a>, it is shown that the stochastic policy gradient converges to Eq. 4 in the limit as the variance of the stochastic policy gradient approaches 0. This is significant because it allows all of the machinery for stochastic policy gradients to be applied to the deterministic policy gradients. I don't talk about the stochastic policy gradient theorem in this post for the sake of brevity, but it is covered in <a href="#References">[3]</a>. 

## Enough with the chit chat, let's see some code!
<a name="Tensorflow"></a> 

Let's get started. 

The first part is easy. Set up a data structure to represent your replay buffer. I recommend using a deque from python's collections lib. The replay buffer will return a randomly chosen batch of experiences when queried.

{% highlight python %}
from collections import deque
import random
import numpy as np

class ReplayBuffer(object):

    def __init__(self, buffer_size):
        self.buffer_size = buffer_size
        self.count = 0
        self.buffer = deque()

    def add(self, s, a, r, t, s2):
        experience = (s, a, r, t, s2)
        if self.count < self.buffer_size: 
            self.buffer.append(experience)
            self.count += 1
        else:
            self.buffer.popleft()
            self.buffer.append(experience)

    def size(self):
        return self.count

    def sample_batch(self, batch_size):
        '''     
        batch_size specifies the number of experiences to add 
        to the batch. If the replay buffer has less than batch_size
        elements, simply return all of the elements within the buffer.
        Generally, you'll want to wait until the buffer has at least 
        batch_size elements before beginning to sample from it.
        '''
        batch = []

        if self.count < batch_size:
            batch = random.sample(self.buffer, self.count)
        else:
            batch = random.sample(self.buffer, batch_size)

        s_batch = np.array([_[0] for _ in batch])
        a_batch = np.array([_[1] for _ in batch])
        r_batch = np.array([_[2] for _ in batch])
        t_batch = np.array([_[3] for _ in batch])
        s2_batch = np.array([_[4] for _ in batch])

        return s_batch, a_batch, r_batch, t_batch, s2_batch

    def clear(self):
        self.deque.clear()
        self.count = 0
{% endhighlight %}

Okay, lets define our actor and critic networks. We're going to use tflearn to condense the boilerplate code. 

{% highlight python %}

...

def create_actor_network(self): 
    inputs = tflearn.input_data(shape=[None, self.s_dim])
    net = tflearn.fully_connected(inputs, 400, activation='relu')
    net = tflearn.fully_connected(net, 300, activation='relu')
    # Final layer weights are init to Uniform[-3e-3, 3e-3]
    w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
    out = tflearn.fully_connected(net, self.a_dim, activation='tanh', weights_init=w_init)
    scaled_out = tf.mul(out, self.action_bound) # Scale output to -action_bound to action_bound
    return inputs, out, scaled_out 

...

def create_critic_network(self):
    inputs = tflearn.input_data(shape=[None, self.s_dim])
    action = tflearn.input_data(shape=[None, self.a_dim])
    net = tflearn.fully_connected(inputs, 400, activation='relu')
    # Add the action tensor in the 2nd hidden layer
    # Use two temp layers to get the corresponding weights and biases
    t1 = tflearn.fully_connected(net, 300)
    t2 = tflearn.fully_connected(action, 300)
    net = tflearn.activation(tf.matmul(net,t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')
    # linear layer connected to 1 output representing Q(s,a) 
    # Weights are init to Uniform[-3e-3, 3e-3]
    w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)
    out = tflearn.fully_connected(net, 1, weights_init=w_init)
    return inputs, action, out

{% endhighlight %}

I would suggest placing these two functions in separate Actor and Critic classes. For the actor network, the output is a tanh layer scaled to be between $[-b, +b], b \in \mathbb{R}$. This is useful when your action space is bounded and closed. 

Notice that the critic network takes both the state and the action as inputs; however, the action input skips the first layer. This is a design decision that has experimentally worked well.

Use Tensorflow placeholders, created by ```tflean.input_data```, for the inputs. Leaving the first dimension of the placeholders as ```None``` allows you to train on batches of experiences.

You can simply call these methods twice, once to create the actor and critic networks to train, and again to create your target actor and critic networks. 

You can create a Tensorflow Op to update the target network parameters like so: 
{% highlight python %}

 self.target_network_params = tf.trainable_variables()[len(self.network_params):]

# Op for periodically updating target network with online network weights
self.update_target_network_params = \
    [self.target_network_params[i].assign(tf.mul(self.network_params[i], self.tau) + \
        tf.mul(self.target_network_params[i], 1. - self.tau))
        for i in range(len(self.target_network_params))]

{% endhighlight %}

The code looks a bit convoluted, but it's actually really clever. You're defining an Op, ```update_target_network_params```, that will copy the parameters of the online network with a mixing factor $\tau$. Make sure you're copying over the correct Tensorflow variables by checking what is being returned by ```tf.trainable_variables()```. You'll need to define this Op for both the actor and critic networks. 

Let's define the gradient computation and optimization Tensorflow operations. 
We'll use [ADAM](https://arxiv.org/abs/1412.6980) as our optimization method. It's sort of become a de-facto standard, in place of SGD.

For the actor network...

{% highlight python %}

# This gradient will be provided by the critic network
self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])

# Combine the gradients here 
self.actor_gradients = tf.gradients(self.scaled_out, self.network_params, -self.action_gradient)

# Optimization Op
self.optimize = tf.train.AdamOptimizer(self.learning_rate).\
    apply_gradients(zip(self.actor_gradients, self.network_params))

{% endhighlight %}

Notice how the gradients are combined. ```tf.gradients()``` makes it quite easy to implement the Deterministic Policy Gradient equation (Eq. 4). I negate the action gradient since you want the policy to do gradient ascent, not descent. Tensorflow will take the sum and average of the gradients of your minibatch.

Then, for the critic network...

{% highlight python %}

# Network target (y_i)
# Obtained from the target networks
self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])

# Define loss and optimization Op
self.loss = tflearn.mean_square(self.predicted_q_value, self.out)
self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)

# Get the gradient of the net w.r.t. the action
self.action_grads = tf.gradients(self.out, self.action)

{% endhighlight %}

This is exactly Eq. 2. Make sure to grab the action gradients at the end there to pass to the policy network for gradient computation. 

I like to encapsulate the calls to my Tensorflow session to keep things clean. For brevity's sake, I'll just show the ones for the actor network.

{% highlight python %}

def train(self, inputs, a_gradient):
    self.sess.run(self.optimize, feed_dict={
        self.inputs: inputs,
        self.action_gradient: a_gradient
    })

def predict(self, inputs):
    return self.sess.run(self.scaled_out, feed_dict={
        self.inputs: inputs
    })

def predict_target(self, inputs):
    return self.sess.run(self.target_out, feed_dict={
        self.target_inputs: inputs
    })

def update_target_network(self):
    self.sess.run(self.update_target_network_params)

{% endhighlight %}

Now, lets show the main training loop and we'll be done! 

{% highlight python %}

... 

with tf.Session() as sess:
        
    env = gym.make('Pendulum-v0')

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = env.action_space.high
    # Ensure action bound is symmetric
    assert (env.action_space.high == -env.action_space.low)

    actor = ActorNetwork(sess, state_dim, action_dim, action_bound, \
        ACTOR_LEARNING_RATE, TAU)

    critic = CriticNetwork(sess, state_dim, action_dim, \
        CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())

    # Initialize our Tensorflow variables
    sess.run(tf.initialize_all_variables())
   
    # Initialize target network weights
    actor.update_target_network()
    critic.update_target_network()

    # Initialize replay memory
    replay_buffer = ReplayBuffer(BUFFER_SIZE)

    for i in xrange(MAX_EPISODES):

        s = env.reset()

        for j in xrange(MAX_EP_STEPS):

            if RENDER_ENV: 
                env.render()

            # Added exploration noise. In the literature, they use
            # Ornstein-Uhlenbeck stochastic process for problem domains
            # with momentum
            a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i + j))

            s2, r, terminal, info = env.step(a[0])

            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r, \
                terminal, np.reshape(s2, (actor.s_dim,)))

            # Keep adding experience to the memory until
            # there are at least minibatch size samples
            if replay_buffer.size() > MINIBATCH_SIZE:     
                s_batch, a_batch, r_batch, t_batch, s2_batch = \
                    replay_buffer.sample_batch(MINIBATCH_SIZE)

                # Calculate targets
                target_q = critic.predict_target(s2_batch, actor.predict_target(s2_batch))

                y_i = []
                for k in xrange(MINIBATCH_SIZE):
                    if t_batch[k]:
                        y_i.append(r_batch[k])
                    else:
                        y_i.append(r_batch[k] + GAMMA * target_q[k])

                # Update the critic given the targets
                predicted_q_value, _ = critic.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)))
            
                # Update the actor policy using the sampled gradient
                a_outs = actor.predict(s_batch)                
                grads = critic.action_gradients(s_batch, a_outs)
                actor.train(s_batch, grads[0])

                # Update target networks
                actor.update_target_network()
                critic.update_target_network()

            if terminal:
                break

{% endhighlight %}

You'll want to add book-keeping to the code and arrange things a little more neatly- you can see the full code [here](https://gist.github.com/pemami4911/27a6b38a8a161adeaca0cc7ec1363480).

I used Tensorboard to view the total reward per episode and average max Q.
Tensorflow is a bit low-level, so it's definitely recommended to use tools like tflearn, keras, Tensorboard, etc., on top of it. I am quite pleased with it though; it's no surprise that TF got so popular so fast. 

## Looking Forward

{%
    include image.html
    img="/assets/results.png"
    caption="Results from running the code on Pendulum-v0"
%}

Despite running this code on the courageous CPU of my Macbook Air, it converged relatively quickly to a good solution. Some ways to potentially see better performance:

1. Use a priority algorithm for sampling from the replay buffer instead of uniformly sampling. See my summary [here](http://pemami4911.github.io/paper_summaries/2016/01/26/prioritizing-experience-replay.html).

2. Experiment with different stochastic policies to improve exploration. 

3. Use recurrent networks to capture temporal nuances of the problem domain. 

In [5], they also used convolutional neural networks to tackle problems with higher-dimensional state spaces. They were able to learn good policies on challenging control tasks with just pixel inputs.  

## References 
<a name="References"></a>

1. Sutton, Richard S., and Andrew G. Barto. [Reinforcement learning: An introduction](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html).
2. [Reinforcement learning: An introduction - Fig 6.15](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/figtmp34.png)
3. Silver, et al. [Deterministic Policy Gradients](http://jmlr.org/proceedings/papers/v32/silver14.pdf)
4. Mnih, et al. [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)
5. Lillicrap, et al. [Continuous control with Deep Reinforcement Learning](http://arxiv.org/pdf/1509.02971v2.pdf)

